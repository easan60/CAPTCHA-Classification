{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading opencv_python-4.5.3.56-cp38-cp38-win_amd64.whl (34.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\easan\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.3.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=Path(\"samples/\")\n",
    "images=list(data_dir.glob(\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel=np.ones((3,1),np.uint8)\n",
    "borderType=cv2.BORDER_CONSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(src):\n",
    "    top=int(0.05*src.shape[0])\n",
    "    bottom=top\n",
    "    left=int(0.15*src.shape[1])\n",
    "    right=left\n",
    "    des=cv2.copyMakeBorder(src,top,bottom,left+1,right,borderType,None,255)\n",
    "    return cv2.bitwise_not(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for image in images:\n",
    "    im=cv2.imread(str(image),0)\n",
    "    threshold=cv2.adaptiveThreshold(im,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,199,5)\n",
    "    erosion=cv2.dilate(threshold,kernel,iterations=2)\n",
    "    s=str(image)\n",
    "    for i in range(5):\n",
    "        x.append(pad(erosion[:,(30+23*i):(30+23*(i+1))]))\n",
    "        y.append(s[-9+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAAD6CAYAAABZC15NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJtElEQVR4nO3dXYhcdxnH8e/PmDSaNpjVtMQkmF4EsQimEFIhXvgWjVGsCJUGlAqB3CikoGiqV94FhOKNN8GGBqytoS8YSrWmoaEUSs2LsSZu28TQlyXBWFtpehOT+ngxJzKOuzv/2Zl55pyZ3weWmfOfzZ7/hh//eeacPedRRGA2bO8Z9QRsMjholsJBsxQOmqVw0CyFg2Yp+gqapK2SXpJ0VtLuQU3Kxo8WehxN0iLgZWALMAMcBbZHxF/m+jdLdF0sZdmC9mfNcIm33oiIlZ3j7+3jZ24CzkbEOQBJDwG3A3MGbSnLuE2f62OXVndPxcOvzjbez1vnauD1tu2Zaux/SNop6ZikY1e43MfurMn6CZpmGfu/9+GI2BsRGyNi42Ku62N31mT9BG0GWNu2vQY43990bFz1E7SjwHpJN0taAtwJHBzMtGzcLPjDQERclfRd4ElgEbAvIk4PbGY2Vvr51ElEPAE8MaC52BjrK2hWf0+ePznv61/88IaUefgUlKVw0CyFg2YpHDRL4aBZCgfNUjholsJBsxQOmqVw0CyFg2YpHDRL4aBZCgfNUjholsJBsxQOmqVw0CyFg2YpfM3AhMm6RqCTVzRL4aBZCgfNUrhGGzPdruMcFa9olsJBsxQOmqVw0CyFg2YpHDRL0TVokvZJuijpVNvYlKRDks5UjyuGO01rupIV7X5ga8fYbuBwRKwHDlfbZnPqGrSIeAZ4s2P4dmB/9Xw/8LXBTsvGzUJrtJsi4gJA9Xjj4KZk42jop6Ak7QR2Aizl/cPendXUQle0v0laBVA9XpzrG93QwmDhQTsI3FU9vwv4zWCmY+Oq5PDGg8BzwEclzUjaAewBtkg6Q6u73Z7hTtOarmuNFhHb53jJbeqsmP8ebcyN6hqBTj4FZSkcNEvhoFkK12gN13mNQF1qsk5e0SyFg2YpHDRL4aBZCgfNUjholsJBsxQOmqXwAduGq+sB2k5e0SyFg2YpHDRL4RqtBxk3uetWczXlJHonr2iWwkGzFA6apWhUjdatRuq1Xun15/VaPy1EXW923C+vaJbCQbMUDpqlaFSN1lkjdatnxrXeaSKvaJbCQbMUDpqlaFSN1o1rsvryimYpHDRLUXLHx7WSnpY0Lem0pF3VuJtaWDFFxPzf0LoZ8qqIOCHpBuA4rb4C3wbejIg9knYDKyLih/P9rOWaits0uhtF9lrDDeNvvYZdR47679OeioePR8TGzvGShhYXIuJE9fwSMA2sxk0trAc91WiS1gG3As9T2NRC0k5JxyQdu8LlPqdrTVUcNEnXA48Ad0fE26X/zn0GDAqDJmkxrZA9EBGPVsPFTS3MSj51CrgPmI6Ie9teclMLK1ZyZmAz8C3gz5JOVmM/otXE4kDV4OI14I6hzNDGQklDi2cBzfGym1pYkbE615mtjtdY1nFO4FNQlsRBsxQOmqWYqBqtLvXKJPKKZikcNEvhoFmKiarRBq2ONV8d5wRe0SyJg2YpHDRL4RptgHxd6dy8olkKB81SOGiWwkGzFA6apXDQLIWDZikcNEvhoFkKB81SOGiWwuc6a2bUf082rOtCvaJZCgfNUjholsI12ojVrSYbFq9olsJBsxQld3xcKukPkv5U9Rn4STXuPgNWrKRGuwx8NiLeqe5l+6yk3wJfBw639RnYDczbZ2DcDLrH+yDU9bqFkj4DERHvVJuLq6/AfQasB6V35V5U3b/2InAoItxnwHpSFLSIeDciNgBrgE2SPl66A/cZMOjxOFpE/FPSEWArVZ+BiLgwKX0G6lr/NEHJp86Vkj5QPX8f8HngRdxnwHpQsqKtAvZLWkQrmAci4nFJz+E+A1aopM/AC7QajXWO/wP3GbBCPtc5QKM+b1lnPgVlKRw0S+GgWQrXaPOo47nMpvKKZikcNEvhoFkK12htxqEm65xjXc7PekWzFA6apXDQLIVrtHk0oSbrpi41m1c0S+GgWQoHzVJMdI1Wl2NMmUZVd3pFsxQOmqVw0CzFRNVok1iT1YVXNEvhoFkKB81SOGiWwkGzFA6apXDQLMVYH0fzcbP68IpmKRw0S1EctOqGyX+U9Hi17T4DVqyXGm0XMA0sr7Z3U7M+A4OuyepY4zX1OobS27+vAb4M/KJt2H0GrFjpW+fPgB8A/24bc58BK1ZyV+6vABcj4vhCduA+AwZlNdpm4KuStgFLgeWSfskE9hmwhSu5K/c9wD0Akj4NfD8ivinpp7T6C+xhRH0G6lisD1uvv3NdPjz0cxxtD7BF0hlgS7VtNqteW/QcAY5Uz91nwIr5zIClaPRJ9WHfwKQu9c18uv3Ona/7AmIbaw6apXDQLEWja7ReNaHm6lVTfievaJbCQbMUDpqlGKsarSn1yiTyimYpHDRL4aBZCgfNUjholsJBsxQOmqVw0CyFg2YpHDRL4aBZCgfNUjholsJBsxQOmqVw0CyFg2YpHDRL4aBZCgfNUjholsJBsxRFl9tJegW4BLwLXI2IjZKmgF8D64BXgG9ExFvDmaY1XS8r2mciYkNEbKy2rzW0WA8crrbNZtXPW6cbWlix0qAF8HtJxyXtrMbc0MKKld4SYXNEnJd0I3BI0oulO4iIvcBegOWaigXM0cZA0YoWEeerx4vAY8AmqoYWAG5oYd2UtOhZJumGa8+BLwCngIO0GlnAiBpaWHOUvHXeBDwm6dr3/yoififpKHBA0g7gNeCO4U3Tmq6kRc854BOzjLuhhRXzmQFL4aBZCgfNUjholsJBsxQOmqVw0CyFg2YpFJF3nlvS34FXgQ8Bb6TtuHee38J9JCJWdg6mBu2/O5WOtf0BZe14foPnt05L4aBZilEFbe+I9lvK8xuwkdRoNnn81mkpHDRLkRo0SVslvSTprKRaXAcqaZ+ki5JOtY1NSTok6Uz1uGKE81sr6WlJ05JOS9pVtzmWSAuapEXAz4EvAbcA2yXdkrX/edwPbO0Yq9PF0VeB70XEx4BPAt+p/t/qNMeuMle0TcDZiDgXEf8CHqJ1EfJIRcQzwJsdw7W5ODoiLkTEier5JWAaWE2N5lgiM2irgdfbtmeqsToqujg6m6R1wK3A89R0jnPJDJpmGfOxlUKSrgceAe6OiLdHPZ9eZQZtBljbtr0GOJ+4/17U6uJoSYtpheyBiHi0Gq7VHLvJDNpRYL2kmyUtAe6kdRFyHdXm4mi1Lqi9D5iOiHvbXqrNHItERNoXsA14Gfgr8OPMfc8zpweBC8AVWqvuDuCDtD7Jnakep0Y4v0/RKjFeAE5WX9vqNMeSL5+CshQ+M2ApHDRL4aBZCgfNUjholsJBsxQOmqX4D2JZYtW7vs4TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[25]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.reshape(x,(-1,54,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '3', '4', '5', '6', '7', '8', 'b', 'c', 'd', 'e', 'f', 'g',\n",
       "       'm', 'n', 'p', 'w', 'x', 'y'], dtype='<U1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "l=['2', '3', '4', '5', '6', '7', '8', 'b', 'c', 'd', 'e', 'f', 'g','m', 'n', 'p', 'w', 'x', 'y']\n",
    "for j in y:\n",
    "    i=l.index(j)\n",
    "    a=[]\n",
    "    for r in range(19):\n",
    "        if(r==i):\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    a=np.array(a)\n",
    "    train.append(a)\n",
    "train=np.array(train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization,MaxPool2D,Dense,Conv2D,Flatten,Dropout\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (50,30,1)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(19, activation = \"softmax\"))\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(data, train, test_size = 0.1, random_state= 2)\n",
    "X_train=X_train/255.0\n",
    "X_val=X_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = False, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - ETA: 0s - loss: 2.7836 - accuracy: 0.1324WARNING:tensorflow:Model was constructed with shape (None, 50, 30, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50, 30, 1), dtype=tf.float32, name='conv2d_38_input'), name='conv2d_38_input', description=\"created by layer 'conv2d_38_input'\"), but it was called on an input with incompatible shape (None, 54, 30, 1).\n",
      "93/93 [==============================] - 21s 215ms/step - loss: 2.7809 - accuracy: 0.1333 - val_loss: 1.3976 - val_accuracy: 0.5500\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 21s 226ms/step - loss: 1.7328 - accuracy: 0.4542 - val_loss: 0.6837 - val_accuracy: 0.7962\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 22s 231ms/step - loss: 1.1319 - accuracy: 0.6411 - val_loss: 0.4581 - val_accuracy: 0.8423\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 23s 243ms/step - loss: 0.7855 - accuracy: 0.7538 - val_loss: 0.3859 - val_accuracy: 0.8788\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 22s 233ms/step - loss: 0.6274 - accuracy: 0.8063 - val_loss: 0.3499 - val_accuracy: 0.8769\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 23s 244ms/step - loss: 0.5251 - accuracy: 0.8306 - val_loss: 0.2685 - val_accuracy: 0.9077\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 21s 227ms/step - loss: 0.4254 - accuracy: 0.8558 - val_loss: 0.2341 - val_accuracy: 0.9212\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 22s 238ms/step - loss: 0.3652 - accuracy: 0.8807 - val_loss: 0.1887 - val_accuracy: 0.9404\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 21s 229ms/step - loss: 0.3520 - accuracy: 0.8905 - val_loss: 0.1751 - val_accuracy: 0.9500\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 20s 218ms/step - loss: 0.2847 - accuracy: 0.9187 - val_loss: 0.1534 - val_accuracy: 0.9481\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 23s 246ms/step - loss: 0.2905 - accuracy: 0.9070 - val_loss: 0.1635 - val_accuracy: 0.9500\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 22s 233ms/step - loss: 0.2685 - accuracy: 0.9190 - val_loss: 0.1214 - val_accuracy: 0.9538\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 0.2593 - accuracy: 0.9247 - val_loss: 0.1392 - val_accuracy: 0.9558\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 21s 225ms/step - loss: 0.2676 - accuracy: 0.9237 - val_loss: 0.1301 - val_accuracy: 0.9500\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 21s 231ms/step - loss: 0.1932 - accuracy: 0.9406 - val_loss: 0.1119 - val_accuracy: 0.9692\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 0.2011 - accuracy: 0.9423 - val_loss: 0.1310 - val_accuracy: 0.9615\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 21s 224ms/step - loss: 0.1777 - accuracy: 0.9526 - val_loss: 0.1551 - val_accuracy: 0.9519\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 22s 241ms/step - loss: 0.1601 - accuracy: 0.9525 - val_loss: 0.1098 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 22s 239ms/step - loss: 0.1514 - accuracy: 0.9563 - val_loss: 0.0968 - val_accuracy: 0.9635\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 23s 244ms/step - loss: 0.1548 - accuracy: 0.9560 - val_loss: 0.0992 - val_accuracy: 0.9712\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 22s 231ms/step - loss: 0.1187 - accuracy: 0.9647 - val_loss: 0.1033 - val_accuracy: 0.9731\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 23s 248ms/step - loss: 0.1261 - accuracy: 0.9681 - val_loss: 0.1008 - val_accuracy: 0.9712\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 23s 242ms/step - loss: 0.1336 - accuracy: 0.9610 - val_loss: 0.1107 - val_accuracy: 0.9673\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 21s 230ms/step - loss: 0.1201 - accuracy: 0.9665 - val_loss: 0.0859 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 21s 221ms/step - loss: 0.1231 - accuracy: 0.9653 - val_loss: 0.0954 - val_accuracy: 0.9731\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 21s 224ms/step - loss: 0.1080 - accuracy: 0.9716 - val_loss: 0.0817 - val_accuracy: 0.9731\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 20s 209ms/step - loss: 0.1064 - accuracy: 0.9704 - val_loss: 0.0965 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 21s 220ms/step - loss: 0.0756 - accuracy: 0.9817 - val_loss: 0.0919 - val_accuracy: 0.9731\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 23s 243ms/step - loss: 0.0851 - accuracy: 0.9771 - val_loss: 0.0926 - val_accuracy: 0.9731\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 0.0848 - accuracy: 0.9775 - val_loss: 0.0811 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=50),\n",
    "                              epochs =  30, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // 50\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_38 (Conv2D)           (None, 50, 30, 16)        416       \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 50, 30, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 25, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 25, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 25, 15, 32)        12832     \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 25, 15, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 12, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 12, 7, 64)         18496     \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 12, 7, 64)         36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 6, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 19)                4883      \n",
      "=================================================================\n",
      "Total params: 400,771\n",
      "Trainable params: 400,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 15ms/step - loss: 0.0811 - accuracy: 0.9712\n",
      "Accuracy: 97.12\n"
     ]
    }
   ],
   "source": [
    "_, accuracy=model.evaluate(X_val, Y_val)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
